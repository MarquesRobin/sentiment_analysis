{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a4b344-7278-40ae-ac34-c8b00a5ead8c",
   "metadata": {},
   "source": [
    "# **Mini Project 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab955a-3c85-4b17-bbd4-25003c4bcc00",
   "metadata": {},
   "source": [
    "0. Requirements:\n",
    "   \n",
    "   If you do not have the following packages installed, run the command below to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088818d9-da0f-4444-8254-0517b30c0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install nltk\n",
    "# !pip install codecarbon\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67299e6e-3164-4d60-856a-d67b172cc449",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "   \n",
    "    Goal: Load and inspect the IMDb dataset containing movie reviews labeled with positive and negative sentiments.(https://ai.stanford.edu/%7Eamaas/data/sentiment/)\n",
    "    \n",
    "    Task: Read the dataset, store the reviews and their associated sentiments, and explore the dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ac27db4f-7b71-41a4-8d94-1a92cacd5c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords # Importe la liste des \"stop words\" (mots vides) de la bibliothèque NLTK (Natural Language Toolkit)\n",
    "from nltk.stem import PorterStemmer # Importe la classe PorterStemmer de NLTK. Le stemming est un processus qui consiste à réduire les mots à leur racine (stem)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from codecarbon import EmissionsTracker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fd23c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fonctions de chargement et d'affichage des données\n"
     ]
    }
   ],
   "source": [
    "# --- Fonction de chargement des données ---\n",
    "\n",
    "def load_movie_reviews(data_folder):\n",
    "    \"\"\"\n",
    "    Charge les critiques de films à partir d'une structure de dossiers\n",
    "    (pos/ et neg/) et les renvoie sous forme de DataFrame Pandas.\n",
    "\n",
    "    Args:\n",
    "        data_folder: Le chemin vers le dossier principal contenant les\n",
    "                     sous-dossiers 'pos' et 'neg'.\n",
    "\n",
    "    Returns:\n",
    "        Un DataFrame Pandas avec deux colonnes : 'review' (texte de la critique)\n",
    "        et 'sentiment' ('pos' ou 'neg').\n",
    "        Retourne None si une erreur se produit.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    sentiments = []\n",
    "\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        folder_path = os.path.join(data_folder, sentiment)  # Chemin complet vers pos/ ou neg/\n",
    "\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"Erreur : Le dossier '{folder_path}' n'existe pas.\")\n",
    "            return None\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):  # Traiter seulement les fichiers .txt\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:  # 'utf-8' pour gérer les accents\n",
    "                        review_text = f.read()\n",
    "                        reviews.append(review_text)\n",
    "                        sentiments.append(sentiment)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Erreur: Fichier '{file_path}' introuvable (improbable).\")\n",
    "                    return None  # Tu peux choisir de continuer ou d'arrêter ici\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors de la lecture de '{file_path}': {e}\")\n",
    "                    return None\n",
    "\n",
    "    # Crée le DataFrame Pandas\n",
    "    df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
    "    return df\n",
    "\n",
    "# --- Fonctions d'affichage ---\n",
    "\n",
    "def display_dataframe_info(df, num_reviews=1, example_index=0):\n",
    "    \"\"\"Affiche des informations complètes sur le DataFrame, y compris des exemples.\n",
    "\n",
    "    Args:\n",
    "        df: Le DataFrame Pandas à afficher.\n",
    "        num_reviews: Le nombre de premières lignes à afficher (head).\n",
    "        example_index: L'indice de la critique d'exemple à afficher.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"Le DataFrame est vide ou None.\")\n",
    "        return\n",
    "\n",
    "    print(df.head(num_reviews))  # Affiche les n premières lignes\n",
    "    print(\"-\" * 20)\n",
    "    print(df.info())  # Informations générales (types, colonnes, etc.)\n",
    "    print(\"-\" * 20)\n",
    "    print(df['sentiment'].value_counts())  # Nombre de critiques par sentiment\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    if example_index < len(df):\n",
    "        print(f\"\\nExemple de critique (index {example_index}):\")\n",
    "        print(df['review'][example_index])\n",
    "        print(\"Sentiment associé:\", df['sentiment'][example_index])\n",
    "    else:\n",
    "        print(f\"L'index d'exemple {example_index} est en dehors des limites du DataFrame.\")\n",
    "\n",
    "def display_first_reviews(df, num_reviews=5):\n",
    "    \"\"\"Affiche les premières lignes (head) du DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Le DataFrame Pandas à afficher.\n",
    "        num_reviews: Le nombre de premières lignes à afficher.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"Le DataFrame est vide ou None.\")\n",
    "        return\n",
    "\n",
    "    print(df.head(num_reviews))\n",
    "\n",
    "print(\"\\nFonctions de chargement et d'affichage des données\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c476fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données :\n",
      "                                              review sentiment\n",
      "0  Zentropa is the most original movie I've seen ...       pos\n",
      "1  Busy is so amazing! I just loved every word sh...       pos\n",
      "2  Another good Stooge short!Christine McIntyre i...       pos\n",
      "3  This is a complex film that explores the effec...       pos\n",
      "4  This film has a special place in my heart, as ...       pos\n",
      "--------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     25000 non-null  object\n",
      " 1   sentiment  25000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 390.8+ KB\n",
      "None\n",
      "--------------------\n",
      "sentiment\n",
      "pos    12500\n",
      "neg    12500\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "\n",
      "Exemple de critique (index 0):\n",
      "Zentropa is the most original movie I've seen in years. If you like unique thrillers that are influenced by film noir, then this is just the right cure for all of those Hollywood summer blockbusters clogging the theaters these days. Von Trier's follow-ups like Breaking the Waves have gotten more acclaim, but this is really his best work. It is flashy without being distracting and offers the perfect combination of suspense and dark humor. It's too bad he decided handheld cameras were the wave of the future. It's hard to say who talked him away from the style he exhibits here, but it's everyone's loss that he went into his heavily theoretical dogma direction instead.\n",
      "Sentiment associé: pos\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "data_directory = \"database_full/train\"  \n",
    "movie_reviews_df = load_movie_reviews(data_directory)\n",
    "\n",
    "# Vérifier le contenu du DataFrame\n",
    "print(\"Données :\")\n",
    "display_dataframe_info(movie_reviews_df, num_reviews=5, example_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1260a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de critiques positives (avant conversion) : 12500\n",
      "Nombre de critiques négatives (avant conversion) : 12500\n"
     ]
    }
   ],
   "source": [
    "def count_pos_strings(df):\n",
    "    \"\"\"\n",
    "    Compte le nombre de critiques positives ('pos') dans la colonne 'sentiment'\n",
    "    d'un DataFrame, AVANT la conversion en 0/1.\n",
    "\n",
    "    Args:\n",
    "        df: Le DataFrame contenant la colonne 'sentiment'.\n",
    "\n",
    "    Returns:\n",
    "        Le nombre de 'pos', ou None si la colonne 'sentiment' n'existe pas.\n",
    "    \"\"\"\n",
    "    if 'sentiment' not in df.columns:\n",
    "        print(\"Erreur : La colonne 'sentiment' est absente du DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    return (df['sentiment'] == 'pos').sum()\n",
    "\n",
    "def count_neg_strings(df):\n",
    "    \"\"\"\n",
    "    Compte le nombre de critiques negative ('neg') dans la colonne 'sentiment'\n",
    "    d'un DataFrame, AVANT la conversion en 0/1.\n",
    "\n",
    "    Args:\n",
    "        df: Le DataFrame contenant la colonne 'sentiment'.\n",
    "\n",
    "    Returns:\n",
    "        Le nombre de 'pos', ou None si la colonne 'sentiment' n'existe pas.\n",
    "    \"\"\"\n",
    "    if 'sentiment' not in df.columns:\n",
    "        print(\"Erreur : La colonne 'sentiment' est absente du DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    return (df['sentiment'] == 'neg').sum()\n",
    "\n",
    "num_pos = count_pos_strings(movie_reviews_df)\n",
    "if num_pos is not None:\n",
    "     print(\"Nombre de critiques positives (avant conversion) :\", num_pos)\n",
    "\n",
    "num_neg = count_neg_strings(movie_reviews_df)\n",
    "if num_neg is not None:\n",
    "    print(\"Nombre de critiques négatives (avant conversion) :\", num_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d51ac0-9bee-470f-83d8-a124ad0aa846",
   "metadata": {},
   "source": [
    "2. Text Preprocessing:\n",
    "   \n",
    "    Goal: Clean and preprocess the text data to remove noise and prepare it for analysis.\n",
    "    \n",
    "    Task: Remove unnecessary characters (e.g., HTML tags, punctuation), convert text to lowercase, and process words by removing stop words and stemming/lemmatizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1ebde4d1-aa98-450a-b972-ef9a0c2f555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fonction de nettoyage des critiques d'entraînement\n"
     ]
    }
   ],
   "source": [
    "# --- Fonctions de nettoyage des données ---\n",
    "\n",
    "def remove_html_bs(text): # fonction pour supprimer les balises HTML\n",
    "    \"\"\"Supprime les balises HTML (avec BeautifulSoup).\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text(separator=\" \")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du nettoyage HTML : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def remove_special_characters(text): # fonction pour supprimer les caractères spéciaux\n",
    "    \"\"\"Supprime les caractères spéciaux et la ponctuation.\"\"\"\n",
    "    pattern = r\"[^a-zA-ZÀ-ÖØ-öø-ÿ0-9\\s]\"\n",
    "    cleaned_text = re.sub(pattern, \" \", text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def convert_to_lowercase(text): # fonction pour convertir les caractères en minuscules\n",
    "    \"\"\"Convertit une chaîne de caractères en minuscules.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text): # fonction pour supprimer les mots vides\n",
    "    \"\"\"Supprime les mots vides (stop words) en utilisant NLTK.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Important: Utilise 'english'\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words) # Reconstruit la phrase\n",
    "\n",
    "def apply_stemming(text): # fonction pour appliquer le stemming\n",
    "    \"\"\"Applique le stemming (PorterStemmer) de NLTK.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def apply_lemmatization(text): # fonction pour appliquer la lemmatisation\n",
    "    \"\"\"Applique la lemmatisation avec WordNetLemmatizer de NLTK.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "def clean_reviews(df):\n",
    "    \"\"\"\n",
    "    Nettoie un DataFrame de critiques (HTML, caractères spéciaux, minuscules, stop words, stemming/lemmatization).\n",
    "    Modifie le DataFrame en place.\n",
    "    \"\"\"\n",
    "    # Vérifie si le DataFrame est vide\n",
    "    if df.empty:\n",
    "        print(\"Erreur : Le DataFrame est vide. Impossible de le nettoyer.\")\n",
    "        return\n",
    "\n",
    "    # Vérifie si la colonne 'review' existe\n",
    "    if 'review' not in df.columns:\n",
    "        print(\"Erreur : La colonne 'review' est absente du DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Applique les fonctions de nettoyage, en séquence\n",
    "    df['review'] = df['review'].apply(remove_html_bs)\n",
    "    df['review'] = df['review'].apply(remove_special_characters)\n",
    "    df['review'] = df['review'].apply(convert_to_lowercase)\n",
    "    df['review'] = df['review'].apply(remove_stopwords)\n",
    "    df['review'] = df['review'].apply(apply_stemming)  # Optionnel : Stemming\n",
    "    df['review'] = df['review'].apply(apply_lemmatization) # Optionnel : Lemmatization\n",
    "\n",
    "    # La fonction ne retourne rien, car elle modifie le DataFrame directement\n",
    "\n",
    "print(\"\\nFonction de nettoyage des critiques d'entraînement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fa515c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17742/195796097.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donnees nettoyées :\n",
      "                                              review sentiment\n",
      "0  zentropa origin movi seen year like uniqu thri...       pos\n",
      "1  busi amaz love everi word ever done freak geek...       pos\n",
      "2  anoth good stoog short christin mcintyr love e...       pos\n",
      "3  complex film explor effect fordist taylorist m...       pos\n",
      "4  film special place heart caught first time tea...       pos\n"
     ]
    }
   ],
   "source": [
    "# === MAIN ===\n",
    "\n",
    "if movie_reviews_df is not None:\n",
    "    clean_reviews(movie_reviews_df)  # Nettoyage des données\n",
    "    print(\"Donnees nettoyées :\")\n",
    "    display_first_reviews(movie_reviews_df, num_reviews=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3e8a7ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critiques d'entraînement nettoyées :\n",
      "                                              review  sentiment\n",
      "0  zentropa origin movi seen year like uniqu thri...          1\n",
      "1  busi amaz love everi word ever done freak geek...          1\n",
      "2  anoth good stoog short christin mcintyr love e...          1\n",
      "3  complex film explor effect fordist taylorist m...          1\n",
      "4  film special place heart caught first time tea...          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17742/2547310409.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  movie_reviews_df['sentiment'] = movie_reviews_df['sentiment'].replace({'pos': 1, 'neg': 0})\n"
     ]
    }
   ],
   "source": [
    "if movie_reviews_df is not None:\n",
    "    # Conversion des étiquettes en 0 et 1\n",
    "    movie_reviews_df['sentiment'] = movie_reviews_df['sentiment'].replace({'pos': 1, 'neg': 0})\n",
    "\n",
    "if movie_reviews_df is not None:\n",
    "    print(\"Critiques d'entraînement nettoyées :\")\n",
    "    display_first_reviews(movie_reviews_df, num_reviews=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbff49f-7177-4dad-8138-a08ec1420c59",
   "metadata": {},
   "source": [
    "3. Feature Extraction:\n",
    "\n",
    "    Goal: Transform the cleaned text into numerical features for machine learning.\n",
    "   \n",
    "    Task: Use a vectorization technique such as TF-IDF to convert the text into a numerical matrix that captures the importance of each word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b77c2734-32dc-4899-bb4d-85140baecd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fonction de vectorisation TF-IDF ---\n",
    "\n",
    "def vectorize_reviews(df, max_features=5000, ngram_range=(1, 2)):\n",
    "    \"\"\"\n",
    "    Vectorise les critiques en utilisant TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        df: Le DataFrame contenant les critiques nettoyées (colonne 'review').\n",
    "        max_features: Le nombre maximum de features (mots/n-grammes) à conserver.\n",
    "        ngram_range:  La plage de n-grammes à considérer (par défaut, unigrammes et bigrammes).\n",
    "\n",
    "    Returns:\n",
    "        Une matrice TF-IDF (sparse matrix) et le vectorizer utilisé.\n",
    "    \"\"\"\n",
    "    if 'review' not in df.columns:\n",
    "        print(\"Erreur : La colonne 'review' est absente du DataFrame.\")\n",
    "        return None, None\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,  # Limite le nombre de features\n",
    "        ngram_range=ngram_range,   # Utilise des unigrammes et des bigrammes\n",
    "        # On pourrait ajouter d'autres paramètres ici, mais les valeurs par défaut sont généralement bonnes\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['review'])  # Applique la vectorisation\n",
    "    return tfidf_matrix, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3a6494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice TF-IDF (forme) : (25000, 5000)\n",
      "Nombre de features : 5000\n",
      "Quelques features (mots/n-grammes) : ['crimin' 'cring' 'crisi' 'critic' 'crocodil' 'crook' 'cross' 'crowd'\n",
      " 'crucial' 'crude' 'cruel' 'cruis' 'crush' 'crystal' 'cuba' 'cube' 'cue'\n",
      " 'cult' 'cult classic' 'cultur']\n",
      "\n",
      "Matrice TF-IDF (dense, extrait) :\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Nouvelle critique vectorisée (forme): (1, 5000)\n"
     ]
    }
   ],
   "source": [
    "if movie_reviews_df is not None:\n",
    "    # Vectorisation\n",
    "    tfidf_matrix, vectorizer = vectorize_reviews(movie_reviews_df)\n",
    "\n",
    "    if tfidf_matrix is not None:  # Vérifie que la vectorisation a réussi\n",
    "        print(\"\\nMatrice TF-IDF (forme) :\", tfidf_matrix.shape)\n",
    "        # Accéder aux noms des features (mots/n-grammes)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        print(\"Nombre de features :\", len(feature_names))\n",
    "        print(\"Quelques features (mots/n-grammes) :\", feature_names[1000:1020]) #Un exemple\n",
    "\n",
    "        # Convertir la matrice sparse en array dense (pour l'affichage, seulement pour l'exemple!)\n",
    "        dense_matrix = tfidf_matrix.toarray()\n",
    "        print(\"\\nMatrice TF-IDF (dense, extrait) :\\n\", dense_matrix[:2, :10])  # Affiche un petit extrait !\n",
    "\n",
    "        #Si tu veux transformer une nouvelle review avec le vectorizer déjà entrainé:\n",
    "        new_review = \"This movie was absolutely amazing! The acting was superb.\"\n",
    "        cleaned_new_review = remove_html_bs(new_review)\n",
    "        cleaned_new_review = remove_special_characters(cleaned_new_review)\n",
    "        cleaned_new_review = convert_to_lowercase(cleaned_new_review)\n",
    "        cleaned_new_review = remove_stopwords(cleaned_new_review)\n",
    "        cleaned_new_review = apply_lemmatization(cleaned_new_review)\n",
    "\n",
    "        new_review_vectorized = vectorizer.transform([cleaned_new_review]) #Transform, et pas fit_transform\n",
    "        print(\"\\nNouvelle critique vectorisée (forme):\", new_review_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bb4744eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  zentropa origin movi seen year like uniqu thri...          1\n",
      "1  busi amaz love everi word ever done freak geek...          1\n",
      "2  anoth good stoog short christin mcintyr love e...          1\n",
      "3  complex film explor effect fordist taylorist m...          1\n",
      "4  film special place heart caught first time tea...          1\n"
     ]
    }
   ],
   "source": [
    "display_first_reviews(movie_reviews_df, num_reviews=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244b9d2-245c-4f5c-a9f1-cd97084d2aab",
   "metadata": {},
   "source": [
    "4. Model Training:\n",
    "\n",
    "    Goal: Train a machine learning model to classify reviews based on their sentiment.\n",
    "    \n",
    "    Task: Split the dataset into training and testing sets, train a Logistic Regression model, and evaluate its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d389d0b4-232a-4ce4-8b19-c311f6037058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      2485\n",
      "           1       0.88      0.90      0.89      2515\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2178  307]\n",
      " [ 254 2261]]\n"
     ]
    }
   ],
   "source": [
    "# TASK 4: Model Training \n",
    "\n",
    "def train_logistic_regression(X, y):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle de régression logistique.\n",
    "\n",
    "    Args:\n",
    "        X: La matrice TF-IDF (features).\n",
    "        y: Les étiquettes (sentiments, 0 ou 1).\n",
    "\n",
    "    Returns:\n",
    "        Le modèle entraîné.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Division des données en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # test_size = 0.2 : 20% des données pour le test, 80% pour l'entrainement\n",
    "    # random_state = 42 :  Pour la reproductibilité.  Fixe la graine du générateur aléatoire.\n",
    "\n",
    "    # 2. Création du modèle de régression logistique\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)  # Augmente max_iter si besoin\n",
    "    # max_iter : Nombre maximum d'itérations pour la descente de gradient.\n",
    "    # random_state : Pour la reproductibilité.\n",
    "\n",
    "    # 3. Entraînement du modèle\n",
    "    model.fit(X_train, y_train)  # C'est ici que le modèle \"apprend\"\n",
    "\n",
    "    return model, X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "if tfidf_matrix is not None:\n",
    "    # Entraînement du modèle\n",
    "    model, X_train, X_test, y_train, y_test = train_logistic_regression(tfidf_matrix, movie_reviews_df['sentiment'])\n",
    "\n",
    "    # --- Évaluation du modèle ---\n",
    "    y_pred = model.predict(X_test) #Prédictions sur le set de test\n",
    "\n",
    "    #print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred)) #La précision\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred)) #Rapport détaillé\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred)) #Matrice de confusion\n",
    "\n",
    "# TASK 8: Track emissions during model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805c3d1-f7b0-4a88-9eec-38244f446d37",
   "metadata": {},
   "source": [
    "5. Model Evaluation:\n",
    "\n",
    "    Goal: Assess the performance of your model using appropriate metrics.\n",
    "    \n",
    "    Task: Evaluate precision, recall, and F1-score of the Logistic Regression model. Use these metrics to identify the strengths and weaknesses of your system. Visualize the Confusion Matrix to better understand how well the model classifies positive and negative reviews. Additionally, test the model with a new review, preprocess it, make a prediction, and display the result. Example: test it with a new review such as:\n",
    "    \"The movie had great visuals, but the storyline was dull and predictable.\" The expected output might be: Negative Sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51fa9b-37e6-4683-a897-0e457cedef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: Model Evaluation \n",
    "\n",
    "# Classification Report\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "\n",
    "# Test with a new review\n",
    "review = \"The movie had great visuals but the storyline was dull and predictable.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef444de-e61a-43c8-81a1-4a82acf787f2",
   "metadata": {},
   "source": [
    "6. Hyperparameter Tuning:\n",
    "\n",
    "    Goal: Optimize your Logistic Regression model by tuning its hyperparameters.\n",
    "   \n",
    "    Task: Use an optimization method to find the best parameters for your model and improve its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dac172-a9b4-496c-b89c-59dbf05e2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 6: Hyperparameter Tuning \n",
    "\n",
    "# TASK 8: Track emissions during Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd78c01-5d86-48b6-ad27-a31ab8cb060b",
   "metadata": {},
   "source": [
    "7. Learning Curve Analysis:\n",
    "\n",
    "    Goal: Diagnose your model's performance by plotting learning curves.\n",
    "   \n",
    "    Task: Analyze training and validation performance as a function of the training set size to identify underfitting or overfitting issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27b49a-3e3e-44ee-9215-79502caee218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 7: Learning Curve Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c754b47-0e39-4ee2-97a0-80e11275716e",
   "metadata": {},
   "source": [
    "9. Ethical Considerations and Explainability:\n",
    "\n",
    "    Goal: Discuss the ethics in using and deploying your AI-based solution by investigating and implementing suitable explainability methods.\n",
    "    \n",
    "    Task: Understanding how a machine learning model makes predictions is crucial for ensuring transparency, fairness, and accountability in AI deployment. One of the widely used techniques for model explainability is SHAP (SHapley Additive exPlanations), which helps determine how much each feature (word) contributes to a prediction.\n",
    "    In this task, you will use SHAP to analyze the impact of individual words on sentiment classification. This will allow you to visualize which words increase or decrease the probability of a positive or negative sentiment prediction. Additionally, discuss key aspects such as potential biases in the model, fairness in outcomes, and accountability in AI decision-making. You can find more information here: https://shap.readthedocs.io/en/latest/generated/shap.Explanation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6bcdfe-5c11-4bfe-af4a-72d53c9925ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 9: Ethical Considerations & Explainability\n",
    "\n",
    "# Show SHAP summary plot with proper feature names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9633197-d745-4879-acd9-c4ed306dda03",
   "metadata": {},
   "source": [
    "10. Deployment Considerations for Embedded Systems:\n",
    "\n",
    "    Goal: Optimize and convert the trained logistic regression model for deployment on embedded systems like Arduino\n",
    "    \n",
    "    Task: To deploy the trained logistic regression model on a resource-constrained embedded system like an Arduino, we must optimize and convert the model into a format suitable for execution in an environment with limited memory and processing power. Since embedded systems do not support direct execution of machine learning models trained in Python, we extract the model’s learned parameters—namely, the weights and bias—after training. These parameters are then quantized to fixed-point integers to eliminate the need for floating-point calculations, which are inefficient on microcontrollers.\n",
    "    Once quantization is applied, we generate a C++ .h header file containing the model’s coefficients and bias, formatted in a way that allows direct use within an Arduino sketch. The final model is optimized to perform inference using integer arithmetic, making it both lightweight and efficient for deployment on microcontrollers. You can find more information here: https://medium.com/@thommaskevin/tinyml-binomial-logistic-regression-0fdbf00e6765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6df56-5cd7-4894-8b19-321c37bb3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 10: Deployment Considerations (Model Quantization & Export for Arduino)\n",
    "# Extract weights and bias from the trained logistic regression model\n",
    "\n",
    "# Apply quantization (convert to fixed-point representation)\n",
    "\n",
    "# Generate C++ header file for Arduino\n",
    "\n",
    "# Save the header file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEI1092",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
