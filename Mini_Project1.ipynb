{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a4b344-7278-40ae-ac34-c8b00a5ead8c",
   "metadata": {},
   "source": [
    "# **Mini Project 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab955a-3c85-4b17-bbd4-25003c4bcc00",
   "metadata": {},
   "source": [
    "0. Requirements:\n",
    "   \n",
    "   If you do not have the following packages installed, run the command below to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088818d9-da0f-4444-8254-0517b30c0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install nltk\n",
    "# !pip install codecarbon\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67299e6e-3164-4d60-856a-d67b172cc449",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "   \n",
    "    Goal: Load and inspect the IMDb dataset containing movie reviews labeled with positive and negative sentiments.(https://ai.stanford.edu/%7Eamaas/data/sentiment/)\n",
    "    \n",
    "    Task: Read the dataset, store the reviews and their associated sentiments, and explore the dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac27db4f-7b71-41a4-8d94-1a92cacd5c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/Application/miniconda/envs/GEI1092/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords # Importe la liste des \"stop words\" (mots vides) de la bibliothèque NLTK (Natural Language Toolkit)\n",
    "from nltk.stem import PorterStemmer # Importe la classe PorterStemmer de NLTK. Le stemming est un processus qui consiste à réduire les mots à leur racine (stem)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd23c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1: Data Preparation \n",
    "# Load the dataset from the local aclImdb folder\n",
    "\n",
    "def load_movie_reviews(data_folder):\n",
    "    \"\"\"\n",
    "    Charge les critiques de films à partir d'une structure de dossiers\n",
    "    (pos/ et neg/) et les renvoie sous forme de DataFrame Pandas.\n",
    "\n",
    "    Args:\n",
    "        data_folder: Le chemin vers le dossier principal contenant les\n",
    "                     sous-dossiers 'pos' et 'neg'.  Ex: \"aclImdb/train\"\n",
    "\n",
    "    Returns:\n",
    "        Un DataFrame Pandas avec deux colonnes : 'review' (texte de la critique)\n",
    "        et 'sentiment' ('pos' ou 'neg').\n",
    "        Retourne None si une erreur se produit (ex: dossier inexistant).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Initialisation des listes pour stocker les données\n",
    "    reviews = []   # Liste vide pour stocker les textes des critiques\n",
    "    sentiments = []  # Liste vide pour stocker les sentiments ('pos' ou 'neg')\n",
    "\n",
    "    # 2. Boucle sur les deux types de sentiments : 'pos' et 'neg'\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        # 2.a. Construction du chemin complet vers le sous-dossier (pos/ ou neg/)\n",
    "        folder_path = os.path.join(data_folder, sentiment)\n",
    "        #   Exemple: si data_folder = \"aclImdb/train\" et sentiment = \"pos\",\n",
    "        #   folder_path devient \"aclImdb/train/pos\"\n",
    "\n",
    "        # 2.b. Vérification de l'existence du dossier\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"Erreur : Le dossier '{folder_path}' n'existe pas.\")\n",
    "            return None  # Arrête la fonction et retourne None en cas d'erreur\n",
    "\n",
    "        # 2.c. Boucle sur tous les fichiers DANS le sous-dossier courant (pos/ ou neg/)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # 2.c.i. Vérification que le fichier se termine par \".txt\"\n",
    "            if filename.endswith(\".txt\"):\n",
    "                # 2.c.ii. Construction du chemin COMPLET vers le fichier texte\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                #    Exemple: si folder_path = \"aclImdb/train/pos\" et filename = \"123_9.txt\",\n",
    "                #    file_path devient \"aclImdb/train/pos/123_9.txt\"\n",
    "\n",
    "                # 2.c.iii. Ouverture et lecture du fichier (avec gestion des erreurs)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:  # Ouvre en lecture, encodage UTF-8\n",
    "                        review_text = f.read()  # Lit TOUT le contenu du fichier\n",
    "                        reviews.append(review_text)     # Ajoute le texte à la liste des critiques\n",
    "                        sentiments.append(sentiment)   # Ajoute le sentiment ('pos' ou 'neg') à la liste\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    # Cette erreur ne devrait normalement pas arriver, car os.listdir() a déjà listé le fichier.\n",
    "                    print(f\"Erreur: Fichier '{file_path}' introuvable (improbable).\")\n",
    "                    return None  # On pourrait choisir de continuer (continue) au lieu d'arrêter.\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Capture toute autre erreur possible pendant la lecture\n",
    "                    print(f\"Erreur lors de la lecture de '{file_path}': {e}\")\n",
    "                    return None\n",
    "\n",
    "    # 3. Création du DataFrame Pandas (APRÈS avoir parcouru tous les dossiers et fichiers)\n",
    "    df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
    "    #   Crée un tableau avec deux colonnes:\n",
    "    #     - 'review': contient les textes des critiques (qui étaient dans la liste 'reviews')\n",
    "    #     - 'sentiment': contient les sentiments ('pos' ou 'neg') (qui étaient dans la liste 'sentiments')\n",
    "\n",
    "    return df  # Retourne le DataFrame créé\n",
    "\n",
    "\n",
    "# Exemple d'utilisation (en dehors de la fonction):\n",
    "# data_directory = \"aclImdb/train\"  # OU \"aclImdb/test\" -  à adapter selon tes besoins\n",
    "# movie_reviews_df = load_movie_reviews(data_directory)\n",
    "#\n",
    "# if movie_reviews_df is not None:\n",
    "#     print(movie_reviews_df.head())\n",
    "\n",
    "# Specify the path to the train folder in the aclImdb dataset\n",
    "\n",
    "# Inspect the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d51ac0-9bee-470f-83d8-a124ad0aa846",
   "metadata": {},
   "source": [
    "2. Text Preprocessing:\n",
    "   \n",
    "    Goal: Clean and preprocess the text data to remove noise and prepare it for analysis.\n",
    "    \n",
    "    Task: Remove unnecessary characters (e.g., HTML tags, punctuation), convert text to lowercase, and process words by removing stop words and stemming/lemmatizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebde4d1-aa98-450a-b972-ef9a0c2f555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: Text Preprocessing \n",
    "# Remove HTML tags\n",
    "\n",
    "# Remove special characters\n",
    "\n",
    "# Convert to lowercase\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbff49f-7177-4dad-8138-a08ec1420c59",
   "metadata": {},
   "source": [
    "3. Feature Extraction:\n",
    "\n",
    "    Goal: Transform the cleaned text into numerical features for machine learning.\n",
    "   \n",
    "    Task: Use a vectorization technique such as TF-IDF to convert the text into a numerical matrix that captures the importance of each word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b77c2734-32dc-4899-bb4d-85140baecd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: Feature Extraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244b9d2-245c-4f5c-a9f1-cd97084d2aab",
   "metadata": {},
   "source": [
    "4. Model Training:\n",
    "\n",
    "    Goal: Train a machine learning model to classify reviews based on their sentiment.\n",
    "    \n",
    "    Task: Split the dataset into training and testing sets, train a Logistic Regression model, and evaluate its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d389d0b4-232a-4ce4-8b19-c311f6037058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: Model Training \n",
    "\n",
    "# TASK 8: Track emissions during model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805c3d1-f7b0-4a88-9eec-38244f446d37",
   "metadata": {},
   "source": [
    "5. Model Evaluation:\n",
    "\n",
    "    Goal: Assess the performance of your model using appropriate metrics.\n",
    "    \n",
    "    Task: Evaluate precision, recall, and F1-score of the Logistic Regression model. Use these metrics to identify the strengths and weaknesses of your system. Visualize the Confusion Matrix to better understand how well the model classifies positive and negative reviews. Additionally, test the model with a new review, preprocess it, make a prediction, and display the result. Example: test it with a new review such as:\n",
    "    \"The movie had great visuals, but the storyline was dull and predictable.\" The expected output might be: Negative Sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce51fa9b-37e6-4683-a897-0e457cedef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: Model Evaluation \n",
    "\n",
    "# Classification Report\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "\n",
    "# Test with a new review\n",
    "review = \"The movie had great visuals but the storyline was dull and predictable.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef444de-e61a-43c8-81a1-4a82acf787f2",
   "metadata": {},
   "source": [
    "6. Hyperparameter Tuning:\n",
    "\n",
    "    Goal: Optimize your Logistic Regression model by tuning its hyperparameters.\n",
    "   \n",
    "    Task: Use an optimization method to find the best parameters for your model and improve its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8dac172-a9b4-496c-b89c-59dbf05e2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 6: Hyperparameter Tuning \n",
    "\n",
    "# TASK 8: Track emissions during Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd78c01-5d86-48b6-ad27-a31ab8cb060b",
   "metadata": {},
   "source": [
    "7. Learning Curve Analysis:\n",
    "\n",
    "    Goal: Diagnose your model's performance by plotting learning curves.\n",
    "   \n",
    "    Task: Analyze training and validation performance as a function of the training set size to identify underfitting or overfitting issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f27b49a-3e3e-44ee-9215-79502caee218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 7: Learning Curve Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c754b47-0e39-4ee2-97a0-80e11275716e",
   "metadata": {},
   "source": [
    "9. Ethical Considerations and Explainability:\n",
    "\n",
    "    Goal: Discuss the ethics in using and deploying your AI-based solution by investigating and implementing suitable explainability methods.\n",
    "    \n",
    "    Task: Understanding how a machine learning model makes predictions is crucial for ensuring transparency, fairness, and accountability in AI deployment. One of the widely used techniques for model explainability is SHAP (SHapley Additive exPlanations), which helps determine how much each feature (word) contributes to a prediction.\n",
    "    In this task, you will use SHAP to analyze the impact of individual words on sentiment classification. This will allow you to visualize which words increase or decrease the probability of a positive or negative sentiment prediction. Additionally, discuss key aspects such as potential biases in the model, fairness in outcomes, and accountability in AI decision-making. You can find more information here: https://shap.readthedocs.io/en/latest/generated/shap.Explanation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6bcdfe-5c11-4bfe-af4a-72d53c9925ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 9: Ethical Considerations & Explainability\n",
    "\n",
    "# Show SHAP summary plot with proper feature names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9633197-d745-4879-acd9-c4ed306dda03",
   "metadata": {},
   "source": [
    "10. Deployment Considerations for Embedded Systems:\n",
    "\n",
    "    Goal: Optimize and convert the trained logistic regression model for deployment on embedded systems like Arduino\n",
    "    \n",
    "    Task: To deploy the trained logistic regression model on a resource-constrained embedded system like an Arduino, we must optimize and convert the model into a format suitable for execution in an environment with limited memory and processing power. Since embedded systems do not support direct execution of machine learning models trained in Python, we extract the model’s learned parameters—namely, the weights and bias—after training. These parameters are then quantized to fixed-point integers to eliminate the need for floating-point calculations, which are inefficient on microcontrollers.\n",
    "    Once quantization is applied, we generate a C++ .h header file containing the model’s coefficients and bias, formatted in a way that allows direct use within an Arduino sketch. The final model is optimized to perform inference using integer arithmetic, making it both lightweight and efficient for deployment on microcontrollers. You can find more information here: https://medium.com/@thommaskevin/tinyml-binomial-logistic-regression-0fdbf00e6765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c6df56-5cd7-4894-8b19-321c37bb3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 10: Deployment Considerations (Model Quantization & Export for Arduino)\n",
    "# Extract weights and bias from the trained logistic regression model\n",
    "\n",
    "# Apply quantization (convert to fixed-point representation)\n",
    "\n",
    "# Generate C++ header file for Arduino\n",
    "\n",
    "# Save the header file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEI1092",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
