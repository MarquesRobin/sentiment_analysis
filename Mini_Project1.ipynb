{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a4b344-7278-40ae-ac34-c8b00a5ead8c",
   "metadata": {},
   "source": [
    "# **Mini Project 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab955a-3c85-4b17-bbd4-25003c4bcc00",
   "metadata": {},
   "source": [
    "0. Requirements:\n",
    "   \n",
    "   If you do not have the following packages installed, run the command below to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088818d9-da0f-4444-8254-0517b30c0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install nltk\n",
    "# !pip install codecarbon\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67299e6e-3164-4d60-856a-d67b172cc449",
   "metadata": {},
   "source": [
    "1. Data Preparation:\n",
    "   \n",
    "    Goal: Load and inspect the IMDb dataset containing movie reviews labeled with positive and negative sentiments.(https://ai.stanford.edu/%7Eamaas/data/sentiment/)\n",
    "    \n",
    "    Task: Read the dataset, store the reviews and their associated sentiments, and explore the dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27db4f-7b71-41a4-8d94-1a92cacd5c9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcodecarbon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmissionsTracker\n\u001b[0;32m---> 16\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124momw-1.4\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#Pour la lemmatisation\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords # Importe la liste des \"stop words\" (mots vides) de la bibliothèque NLTK (Natural Language Toolkit)\n",
    "from nltk.stem import PorterStemmer # Importe la classe PorterStemmer de NLTK. Le stemming est un processus qui consiste à réduire les mots à leur racine (stem)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from codecarbon import EmissionsTracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd23c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  This was absolutely one of the best movies I'v...       pos\n",
      "1  As others that have commented around the web.....       pos\n",
      "2  The plot of this movie is set against the most...       pos\n",
      "3  The cinematography is the film's shining featu...       pos\n",
      "4  (This has been edited for space)<br /><br />Ch...       pos\n",
      "--------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6250 entries, 0 to 6249\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     6250 non-null   object\n",
      " 1   sentiment  6250 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 97.8+ KB\n",
      "None\n",
      "--------------------\n",
      "sentiment\n",
      "pos    3125\n",
      "neg    3125\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "\n",
      "Exemple de critique :\n",
      "This was absolutely one of the best movies I've seen. <br /><br />Excellent performances from a marvelous A-List cast that will move you from smiles to laughter to tears and back.<br /><br />I couldn't help but care about the characters. Ms. Merkerson will blow you away, as will the young man playing the young lead.<br /><br />I also thought that the set design was top-rate. The viewer is really placed inside each era as it's presented. <br /><br />The music is a blast, too. Nice selections to represent mood, time and place. The blind blues man is stereotypic but he delivers some great songs. <br /><br />This is a great story that will survive many repeated viewings. Take the time to watch it!\n",
      "Sentiment associé: pos\n"
     ]
    }
   ],
   "source": [
    "# TASK 1: Data Preparation \n",
    "# Load the dataset from the local aclImdb folder\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_movie_reviews(data_folder):\n",
    "    \"\"\"\n",
    "    Charge les critiques de films à partir d'une structure de dossiers\n",
    "    (pos/ et neg/) et les renvoie sous forme de DataFrame Pandas.\n",
    "\n",
    "    Args:\n",
    "        data_folder: Le chemin vers le dossier principal contenant les\n",
    "                     sous-dossiers 'pos' et 'neg'.\n",
    "\n",
    "    Returns:\n",
    "        Un DataFrame Pandas avec deux colonnes : 'review' (texte de la critique)\n",
    "        et 'sentiment' ('pos' ou 'neg').\n",
    "        Retourne None si une erreur se produit.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    sentiments = []\n",
    "\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        folder_path = os.path.join(data_folder, sentiment)  # Chemin complet vers pos/ ou neg/\n",
    "\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"Erreur : Le dossier '{folder_path}' n'existe pas.\")\n",
    "            return None\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):  # Important: traiter seulement les fichiers .txt\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:  # 'utf-8' pour gérer les accents\n",
    "                        review_text = f.read()\n",
    "                        reviews.append(review_text)\n",
    "                        sentiments.append(sentiment)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Erreur: Fichier '{file_path}' introuvable (improbable).\")\n",
    "                    return None  # Tu peux choisir de continuer ou d'arrêter ici\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors de la lecture de '{file_path}': {e}\")\n",
    "                    return None\n",
    "\n",
    "    # Crée le DataFrame Pandas\n",
    "    df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})\n",
    "    return df\n",
    "\n",
    "\n",
    "# === Utilisation de la fonction ===\n",
    "\n",
    "# ADAPTE CE CHEMIN !!!\n",
    "# 1. Charge les données pour l'entraînement \n",
    "data_directory = \"database_less/train\"  #  Si le dossier 'aclImdb' est dans le même répertoire que ton notebook.\n",
    "movie_reviews_train_df = load_movie_reviews(data_directory)\n",
    "# 2. Charge les données pour le test\n",
    "data_directory = \"database_less/test\"\n",
    "movie_reviews_test_df= load_movie_reviews(data_directory)\n",
    "\n",
    "# 3. Vérifie que le chargement a réussi et affiche des informations\n",
    "if movie_reviews_test_df is not None:\n",
    "    print(movie_reviews_test_df.head())  # Premières lignes\n",
    "    print(\"-\" * 20)  # Séparateur\n",
    "    print(movie_reviews_test_df.info())  # Infos générales\n",
    "    print(\"-\" * 20)\n",
    "    print(movie_reviews_test_df['sentiment'].value_counts())  # Compte les sentiments\n",
    "    print(\"-\" * 20)\n",
    "    print(\"\\nExemple de critique :\")  #Afficher une critique complète\n",
    "    print(movie_reviews_test_df['review'][0]) #La première\n",
    "    print(\"Sentiment associé:\", movie_reviews_test_df['sentiment'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d51ac0-9bee-470f-83d8-a124ad0aa846",
   "metadata": {},
   "source": [
    "2. Text Preprocessing:\n",
    "   \n",
    "    Goal: Clean and preprocess the text data to remove noise and prepare it for analysis.\n",
    "    \n",
    "    Task: Remove unnecessary characters (e.g., HTML tags, punctuation), convert text to lowercase, and process words by removing stop words and stemming/lemmatizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebde4d1-aa98-450a-b972-ef9a0c2f555a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 75\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# La fonction ne retourne rien, car elle modifie le DataFrame directement\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 2. Nettoyer les données (si le chargement a réussi)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m movie_reviews_test_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mclean_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie_reviews_test_df\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Nettoyage *en place*\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(movie_reviews_test_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExemple de review après le prétraitement complet :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 68\u001b[0m, in \u001b[0;36mclean_reviews\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     66\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(remove_stopwords)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# df['review'] = df['review'].apply(apply_stemming)  # Optionnel : Stemming\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply_lemmatization\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Application/miniconda/envs/GEI1092/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Application/miniconda/envs/GEI1092/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Application/miniconda/envs/GEI1092/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Application/miniconda/envs/GEI1092/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Application/miniconda/envs/GEI1092/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[24], line 42\u001b[0m, in \u001b[0;36mapply_lemmatization\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_lemmatization\u001b[39m(text):\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applique la lemmatisation avec WordNetLemmatizer de NLTK.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     lemmatizer \u001b[38;5;241m=\u001b[39m \u001b[43mWordNetLemmatizer\u001b[49m()\n\u001b[1;32m     43\u001b[0m     words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     44\u001b[0m     lemmatized_words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "# TASK 2: Text Preprocessing \n",
    "# Remove HTML tags\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_bs(text):\n",
    "    \"\"\"Supprime les balises HTML (avec BeautifulSoup).\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        return soup.get_text(separator=\" \")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du nettoyage HTML : {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"Supprime les caractères spéciaux et la ponctuation.\"\"\"\n",
    "    pattern = r\"[^a-zA-ZÀ-ÖØ-öø-ÿ0-9\\s]\"\n",
    "    cleaned_text = re.sub(pattern, \" \", text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    \"\"\"Convertit une chaîne de caractères en minuscules.\"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Supprime les mots vides (stop words) en utilisant NLTK.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Important: Utilise 'english'\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words) # Reconstruit la phrase\n",
    "\n",
    "def apply_stemming(text):\n",
    "    \"\"\"Applique le stemming (PorterStemmer) de NLTK.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def apply_lemmatization(text):\n",
    "    \"\"\"Applique la lemmatisation avec WordNetLemmatizer de NLTK.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "def clean_reviews(df):\n",
    "    \"\"\"\n",
    "    Nettoie un DataFrame de critiques (HTML, caractères spéciaux, minuscules, stop words, stemming/lemmatization).\n",
    "    Modifie le DataFrame en place.\n",
    "    \"\"\"\n",
    "    # Vérifie si le DataFrame est vide\n",
    "    if df.empty:\n",
    "        print(\"Erreur : Le DataFrame est vide. Impossible de le nettoyer.\")\n",
    "        return\n",
    "\n",
    "    # Vérifie si la colonne 'review' existe\n",
    "    if 'review' not in df.columns:\n",
    "        print(\"Erreur : La colonne 'review' est absente du DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Applique les fonctions de nettoyage, en séquence\n",
    "    df['review'] = df['review'].apply(remove_html_bs)\n",
    "    df['review'] = df['review'].apply(remove_special_characters)\n",
    "    df['review'] = df['review'].apply(convert_to_lowercase)\n",
    "    df['review'] = df['review'].apply(remove_stopwords)\n",
    "    # df['review'] = df['review'].apply(apply_stemming)  # Optionnel : Stemming\n",
    "    # df['review'] = df['review'].apply(apply_lemmatization) # Optionnel : Lemmatization\n",
    "\n",
    "    # La fonction ne retourne rien, car elle modifie le DataFrame directement\n",
    "\n",
    "\n",
    "# 2. Nettoyer les données (si le chargement a réussi)\n",
    "if movie_reviews_test_df is not None:\n",
    "    clean_reviews(movie_reviews_test_df)  # Nettoyage *en place*\n",
    "    print(movie_reviews_test_df.head())\n",
    "    print(\"\\nExemple de review après le prétraitement complet :\")\n",
    "    print(movie_reviews_test_df['review'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbff49f-7177-4dad-8138-a08ec1420c59",
   "metadata": {},
   "source": [
    "3. Feature Extraction:\n",
    "\n",
    "    Goal: Transform the cleaned text into numerical features for machine learning.\n",
    "   \n",
    "    Task: Use a vectorization technique such as TF-IDF to convert the text into a numerical matrix that captures the importance of each word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b77c2734-32dc-4899-bb4d-85140baecd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: Feature Extraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244b9d2-245c-4f5c-a9f1-cd97084d2aab",
   "metadata": {},
   "source": [
    "4. Model Training:\n",
    "\n",
    "    Goal: Train a machine learning model to classify reviews based on their sentiment.\n",
    "    \n",
    "    Task: Split the dataset into training and testing sets, train a Logistic Regression model, and evaluate its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d389d0b4-232a-4ce4-8b19-c311f6037058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: Model Training \n",
    "\n",
    "# TASK 8: Track emissions during model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805c3d1-f7b0-4a88-9eec-38244f446d37",
   "metadata": {},
   "source": [
    "5. Model Evaluation:\n",
    "\n",
    "    Goal: Assess the performance of your model using appropriate metrics.\n",
    "    \n",
    "    Task: Evaluate precision, recall, and F1-score of the Logistic Regression model. Use these metrics to identify the strengths and weaknesses of your system. Visualize the Confusion Matrix to better understand how well the model classifies positive and negative reviews. Additionally, test the model with a new review, preprocess it, make a prediction, and display the result. Example: test it with a new review such as:\n",
    "    \"The movie had great visuals, but the storyline was dull and predictable.\" The expected output might be: Negative Sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce51fa9b-37e6-4683-a897-0e457cedef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: Model Evaluation \n",
    "\n",
    "# Classification Report\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "\n",
    "# Test with a new review\n",
    "review = \"The movie had great visuals but the storyline was dull and predictable.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef444de-e61a-43c8-81a1-4a82acf787f2",
   "metadata": {},
   "source": [
    "6. Hyperparameter Tuning:\n",
    "\n",
    "    Goal: Optimize your Logistic Regression model by tuning its hyperparameters.\n",
    "   \n",
    "    Task: Use an optimization method to find the best parameters for your model and improve its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8dac172-a9b4-496c-b89c-59dbf05e2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 6: Hyperparameter Tuning \n",
    "\n",
    "# TASK 8: Track emissions during Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd78c01-5d86-48b6-ad27-a31ab8cb060b",
   "metadata": {},
   "source": [
    "7. Learning Curve Analysis:\n",
    "\n",
    "    Goal: Diagnose your model's performance by plotting learning curves.\n",
    "   \n",
    "    Task: Analyze training and validation performance as a function of the training set size to identify underfitting or overfitting issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f27b49a-3e3e-44ee-9215-79502caee218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 7: Learning Curve Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c754b47-0e39-4ee2-97a0-80e11275716e",
   "metadata": {},
   "source": [
    "9. Ethical Considerations and Explainability:\n",
    "\n",
    "    Goal: Discuss the ethics in using and deploying your AI-based solution by investigating and implementing suitable explainability methods.\n",
    "    \n",
    "    Task: Understanding how a machine learning model makes predictions is crucial for ensuring transparency, fairness, and accountability in AI deployment. One of the widely used techniques for model explainability is SHAP (SHapley Additive exPlanations), which helps determine how much each feature (word) contributes to a prediction.\n",
    "    In this task, you will use SHAP to analyze the impact of individual words on sentiment classification. This will allow you to visualize which words increase or decrease the probability of a positive or negative sentiment prediction. Additionally, discuss key aspects such as potential biases in the model, fairness in outcomes, and accountability in AI decision-making. You can find more information here: https://shap.readthedocs.io/en/latest/generated/shap.Explanation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6bcdfe-5c11-4bfe-af4a-72d53c9925ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 9: Ethical Considerations & Explainability\n",
    "\n",
    "# Show SHAP summary plot with proper feature names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9633197-d745-4879-acd9-c4ed306dda03",
   "metadata": {},
   "source": [
    "10. Deployment Considerations for Embedded Systems:\n",
    "\n",
    "    Goal: Optimize and convert the trained logistic regression model for deployment on embedded systems like Arduino\n",
    "    \n",
    "    Task: To deploy the trained logistic regression model on a resource-constrained embedded system like an Arduino, we must optimize and convert the model into a format suitable for execution in an environment with limited memory and processing power. Since embedded systems do not support direct execution of machine learning models trained in Python, we extract the model’s learned parameters—namely, the weights and bias—after training. These parameters are then quantized to fixed-point integers to eliminate the need for floating-point calculations, which are inefficient on microcontrollers.\n",
    "    Once quantization is applied, we generate a C++ .h header file containing the model’s coefficients and bias, formatted in a way that allows direct use within an Arduino sketch. The final model is optimized to perform inference using integer arithmetic, making it both lightweight and efficient for deployment on microcontrollers. You can find more information here: https://medium.com/@thommaskevin/tinyml-binomial-logistic-regression-0fdbf00e6765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c6df56-5cd7-4894-8b19-321c37bb3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 10: Deployment Considerations (Model Quantization & Export for Arduino)\n",
    "# Extract weights and bias from the trained logistic regression model\n",
    "\n",
    "# Apply quantization (convert to fixed-point representation)\n",
    "\n",
    "# Generate C++ header file for Arduino\n",
    "\n",
    "# Save the header file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GEI1092",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
